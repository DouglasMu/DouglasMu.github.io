I"o<ul id="markdown-toc">
  <li><a href="#预训练模型总结" id="markdown-toc-预训练模型总结">预训练模型总结</a>    <ul>
      <li><a href="#什么是预训练模型" id="markdown-toc-什么是预训练模型">什么是预训练模型？</a></li>
      <li><a href="#预训练模型有什么作用" id="markdown-toc-预训练模型有什么作用">预训练模型有什么作用？</a></li>
      <li><a href="#预训练模型的两大范式浅层词嵌入和预训练编码器" id="markdown-toc-预训练模型的两大范式浅层词嵌入和预训练编码器">预训练模型的两大范式（浅层词嵌入和预训练编码器）</a>        <ul>
          <li><a href="#浅层词嵌入" id="markdown-toc-浅层词嵌入">浅层词嵌入</a></li>
          <li><a href="#预训练编码器" id="markdown-toc-预训练编码器">预训练编码器</a></li>
        </ul>
      </li>
      <li><a href="#按照类型进行分类" id="markdown-toc-按照类型进行分类">按照类型进行分类</a>        <ul>
          <li><a href="#自监督模型的分类" id="markdown-toc-自监督模型的分类">自监督模型的分类</a>            <ul>
              <li><a href="#基于上下文context-based" id="markdown-toc-基于上下文context-based">基于上下文（Context Based）</a></li>
              <li><a href="#基于对比contrastive-based" id="markdown-toc-基于对比contrastive-based">基于对比（Contrastive Based）</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#ptms有哪些拓展" id="markdown-toc-ptms有哪些拓展">PTMs有哪些拓展？</a>        <ul>
          <li><a href="#1引入知识" id="markdown-toc-1引入知识">1、引入知识</a></li>
          <li><a href="#2模型压缩" id="markdown-toc-2模型压缩">2、模型压缩</a></li>
          <li><a href="#3多模态" id="markdown-toc-3多模态">3、多模态</a></li>
          <li><a href="#4领域预训练" id="markdown-toc-4领域预训练">4、领域预训练</a></li>
          <li><a href="#5多语言和特定语言" id="markdown-toc-5多语言和特定语言">5、多语言和特定语言</a></li>
        </ul>
      </li>
      <li><a href="#对ptms进行迁移学习" id="markdown-toc-对ptms进行迁移学习">对PTMs进行迁移学习</a>        <ul>
          <li><a href="#如何迁移" id="markdown-toc-如何迁移">如何迁移？</a></li>
          <li><a href="#fine-tune策略" id="markdown-toc-fine-tune策略">fine-tune策略：</a></li>
        </ul>
      </li>
      <li><a href="#比较典型的基于语言模型的预训练模型发展脉络" id="markdown-toc-比较典型的基于语言模型的预训练模型发展脉络">比较典型的基于语言模型的预训练模型发展脉络</a>        <ul>
          <li><a href="#elmodeep-contextualized-word-representations" id="markdown-toc-elmodeep-contextualized-word-representations">ELMo:Deep contextualized word representations</a></li>
          <li><a href="#gptimproving-language-understandingby-generative-pre-training" id="markdown-toc-gptimproving-language-understandingby-generative-pre-training">GPT:Improving Language Understandingby Generative Pre-Training</a></li>
          <li><a href="#gpt20language-models-are-unsupervised-multitask-learners" id="markdown-toc-gpt20language-models-are-unsupervised-multitask-learners">GPT2.0:Language Models are Unsupervised Multitask Learners</a></li>
          <li><a href="#gpt30-language-models-are-few-shot-learners" id="markdown-toc-gpt30-language-models-are-few-shot-learners">GPT3.0 Language Models are Few-Shot Learners</a></li>
          <li><a href="#bert-pre-training-of-deep-bidirectional-transformers-for-lanuage-understanding" id="markdown-toc-bert-pre-training-of-deep-bidirectional-transformers-for-lanuage-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Lanuage Understanding</a></li>
          <li><a href="#massmasked-sequence-to-sequence-pre-training-for-language-generation" id="markdown-toc-massmasked-sequence-to-sequence-pre-training-for-language-generation">MASS:Masked Sequence to Sequence Pre-training for Language Generation</a></li>
          <li><a href="#unilmunified-language-model-pre-training" id="markdown-toc-unilmunified-language-model-pre-training">UniLM:Unified Language Model Pre-training</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="预训练模型总结">预训练模型总结</h2>

<p><img src="https://i.loli.net/2020/10/06/CzZjFTuVW7ceoaJ.png" alt="自然语言处理流程图.png" /></p>

<h3 id="什么是预训练模型">什么是预训练模型？</h3>

<ul>
  <li>
    <p>迁移学习(Transfer learning) 顾名思义就是把已训练好的模型（预训练模型）参数
迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务都是存在相关性的，
所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）
通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。</p>
  </li>
  <li>
    <p>pre-training就是把一个已经训练好的模型参数应用到另一个类似任务上作为初始参数，
这样之前训练模型的过程就叫做预训练</p>
  </li>
  <li>
    <p>fine tuning就是在训练新任务的过程中慢慢调整参数（微调）</p>
  </li>
</ul>

<h3 id="预训练模型有什么作用">预训练模型有什么作用？</h3>

<p>深度学习时代，为了充分训练深层模型参数并防止过拟合，通常需要更多标注数据喂养。
在NLP领域，标注数据更是一个昂贵资源。PTMs从大量无标注数据中进行预训练使许多NLP任务获得显著的性能提升。
总的来看，预训练模型PTMs的优势包括：</p>

<ul>
  <li>在庞大的无标注数据上进行预训练可以获取更通用的语言表示，并有利于下游任务；</li>
  <li>为模型提供了一个更好的初始化参数，在目标任务上具备更好的泛化性能、并加速收敛；</li>
  <li>是一种有效的正则化手段，避免在小数据集上过拟合（一个随机初始化的深层模型容易对小数据集过拟合）</li>
</ul>

<h3 id="预训练模型的两大范式浅层词嵌入和预训练编码器">预训练模型的两大范式（浅层词嵌入和预训练编码器）</h3>

<h4 id="浅层词嵌入">浅层词嵌入</h4>

<p>这一类PTMs范式是我们通常所说的“词向量”，
其主要特点是学习到的是上下文独立的静态词嵌入，
其主要代表为NNLM、word2vec、Glove等。
这一类词嵌入通常采取浅层网络进行训练，而应用于下游任务时，整个模型的其余部分仍需要从头开始学习。
因此，对于这一范式的PTMs没有必要采取深层神经网络进行训练，采取浅层网络加速训练也可以产生好的词嵌入。</p>

<p>浅层词嵌入的主要缺陷为：</p>

<ul>
  <li>词嵌入与上下文无关，每个单词的嵌入向量始终是相同，因此不能解决一词多义的问题。</li>
  <li>通常会出现OOV问题(Out-of-vocabulary)，为了解决这个问题，相关文献提出了字符级表示或sub-word表示，
如CharCNN、FastText和 Byte-Pair Encoding。</li>
</ul>

<h4 id="预训练编码器">预训练编码器</h4>
<p>第二类PTMs范式为预训练编码器，主要目的是通过一个预训练的编码器能够输出上下文相关的词向量，
解决一词多义的问题。这一类预训练编码器输出的向量称之为「上下文相关的词嵌入」。</p>

<p>PTMs中预训练编码器通常采用LSTM和Transformer（Transformer-XL）。
其中Transformer又依据其attention-mask方式
分为Transformer-Encoder和Transformer-Decoder两部分。
此外，Transformer也可看作是一种图神经网络GNN。
这一类「预训练编码器」范式的PTMs主要代表有ELMO、GPT-1、BERT、XLNet等。</p>

<h3 id="按照类型进行分类">按照类型进行分类</h3>

<p>按照类型可以分成三大类：监督学习、无监督学习和自监督学习</p>
<ul>
  <li>
    <p>监督学习(Supervised learning, SL)是基于 input-output对组成的训练数据，学习将输入映射到输出的函数。(CoVe)</p>
  </li>
  <li>
    <p>无监督学习(UL)是从未标记的数据中发现一些内在的知识，如簇(clusters)、密度(densities)、
潜在表示(latent representation)。</p>
  </li>
  <li>
    <p>自监督学习(SSL)是监督学习和无监督学习的混合。SSL的学习模式与监督学习完全相同，
但是训练数据的标签是自动生成的。SSL的关键思想是以某种形式从其他部分输入预测任一部分的输入。
例如，掩蔽语言模型(masked language model，MLM)是一个自监督的任务，
它尝试通过一个句子中其余词去预测被MASK的词。</p>
  </li>
</ul>

<h4 id="自监督模型的分类">自监督模型的分类</h4>
<h5 id="基于上下文context-based">基于上下文（Context Based）</h5>
<ul>
  <li>自回归语言模型（LM）</li>
</ul>

<p>优点：</p>

<p>语言模型（language model，LM）联合概率的无偏估计，即为传统的语言模型，考虑被预测单词之间的相关性，
天然适合处理自然生成任务；</p>

<p>缺点：</p>

<p>联合概率按照文本序列顺序拆解（从左至右分解），无法获取双向上下文信息表征；
代表模型：ELMO、GPT-1、GPT-2、ULMFiT、SiATL；</p>

<ul>
  <li>自编码语言模型（DAE）</li>
</ul>

<p>优点：</p>

<p>本质为降噪自编码(DAE)特征表示，通过引入噪声[MASK]构建MLM(Masked language model)，
获取双向上下文信息表征（本文将自编码语言模型统一称为DAE，旨在采用部分损坏的输入，
旨在恢复原始的未失真输入）；如果当前token被预测，否则为原始文本被替换后的输入。</p>

<p>缺点：</p>

<p>引入独立性假设，为语言模型联合概率的有偏估计，没有考虑预测token之间的相关性；
预训练时的「MASK」噪声在finetune阶段不会出现，造成两阶段不匹配问题；
为解决这一问题，在15%被预测的token中，80%被替换为「MASK」，10%被随机替换，10%被替换为原词。</p>

<p>代表模型：BERT、MASS、T5、RoBERTa、UniLM、XLM、SpanBERT、ERNIE-Baidu、E-BERT、ERNIE-THU、BART。</p>

<p>BERT是自编码语言模型的一个典型代表，但其采用的MLM策略和Transformer-Encoder结构，
导致其不适合直接处理生成任务。为了解决这一问题，也可采用基于Seq2Seq MLM方法：encoder部分采取masked策略，
而decoder部分以自回归的方式预测encoder部分被mask的token。
此外，还有很多基于自编码语言模型的PTMs提出了不同的MLM增强策略，
称之为 Enhanced Masked Language Modeling (E-MLM)。</p>

<ul>
  <li>排列语言模型（PLM）</li>
</ul>

<p>排列语言模型综合了LM和DAE-LM两者的优点。严格来讲，PLM和LM是标准的自回归语言模型
（注：PLM是一种广义的自回归方法），而MLM不是一个标准的语言模型，其引入独立性假设，
隐式地学习预测token（mask部分本身的强相关性）之间的关系。如果衡量序列中被建模的依赖关系的数量，
标准的自回归语言模型可以达到上界，不依赖于任何独立假设。
LM和PLM能够通过自回归方式来显式地学习预测token之间的关系。
然而，LM无法对双向上下文进行表征，借鉴 NADE的思想，PLM将这种传统的自回归语言模型（LM）进行推广，
将顺序拆解变为随机拆解（从左至右分解），产生上下文相关的双向特征表示。</p>

<p>PLM最为典型的代表就是XLNet，这是对标准语言模型的一个复兴，
提出一个框架来连接标准语言模型建模方法和预训练方法。</p>

<h5 id="基于对比contrastive-based">基于对比（Contrastive Based）</h5>
<ul>
  <li>Deep InfoMax (DIM)</li>
</ul>

<p>DIM方法来源于CV领域，对于全局的特征（编码器最终的输出）和局部特征（编码器中间层的特征），
DIM需要判断全局特征和局部特征是否来自同一图像。</p>

<p>InfoWord将DIM引入到NLP中，用Mutual Information的一个下界InfoNCE来重新解释BERT和XLNET的objective，
并提出一个新的DIM objective以最大化一个句子的global representation和其中一个ngram的local representation之间
的Mutual Information。</p>

<ul>
  <li>Replaced Token Detection (RTD)</li>
</ul>

<p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，
可以很好的训练词嵌入。RTD于与 NCE 相同，根据上下文语境来预测token是否替换 。</p>

<p>word2vec中的negative sampling可看作是RTD，负样本从词表中进行带权采样。</p>

<p>ELECTRA提出了一种新的预训练任务框架，构建生成器-判别器，生成器通过MLM任务对被mask的token进行预测，
迭代器判断原始句子中的每个token是否被replace过。生成器相当于对输入进行了筛选，
使判别器的任务更难，从而学习到更好的表示。生成器-判别器共享embedding，生成器部分采用small-bert，
判别器部分对每一个token采用sigmoid计算loss。finetune阶段只采用判别器部分。
RTD也被看作解决MLM中「MASK」在预训练和finetune间差异的一种手段。</p>

<p>WKLM在实体level进行替换，替换为具有相同实体类型的实体名称。</p>

<ul>
  <li>Next Sentence Prediction (NSP)</li>
  <li>Sentence Order Prediction (SOP)
    <h3 id="ptms有哪些拓展">PTMs有哪些拓展？</h3>
  </li>
</ul>

<h4 id="1引入知识">1、引入知识</h4>

<p>PTMs通常从通用大型文本语料库中学习通用语言表示，但是缺少特定领域的知识。PTMs中设计一些辅助的预训练任务，
将外部知识库中的领域知识整合到PTMs中被证明是有效的。</p>

<p>ERNIE-THU将在知识图谱中预先训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。
由于语言表征的预训练过程和知识表征过程有很大的不同，会产生两个独立的向量空间。为解决上述问题，
在有实体输入的位置，将实体向量和文本表示通过非线性变换进行融合，以融合词汇、句法和知识信息。
LIBERT（语言知识的BERT）通过附加的语言约束任务整合了语言知识。
SentiLR集成了每个单词的情感极性，以将MLM扩展到标签感知MLM（LA-MLM），ABSA任务上都达到SOTA。
SenseBERT 不仅能够预测被mask的token，还能预测它们在给定语境下的实际含义。
使用英语词汇数据库 WordNet 作为标注参照系统，预测单词在语境中的实际含义，显著提升词汇消歧能力。
KnowBERT 与实体链接模型以端到端的方式合并实体表示。</p>

<p>KG-BERT显示输入三元组形式，采取两种方式进行预测：构建三元组识别和关系分类，
共同优化知识嵌入和语言建模目标。这些工作通过实体嵌入注入知识图的结构信息。
K-BERT将从KG提取的相关三元组显式地注入句子中，以获得BERT的扩展树形输入。
K-Adapter通过针对不同的预训练任务独立地训练不同的适配器来注入多种知识，从而可以不断地注入知识，
以解决注入多种知识时可能会出现灾难性遗忘问题。
此外，这类PTMs还有WKLM、KEPLER和等。</p>
<h4 id="2模型压缩">2、模型压缩</h4>

<p>由于预训练的语言模型通常包含至少数亿个参数，
因此很难将它们部署在现实应用程序中的在线服务和资源受限的设备上。
模型压缩是减小模型尺寸并提高计算效率的有效方法。</p>

<p>5种PTMs的压缩方法为：</p>

<ul>
  <li>pruning（剪枝）：将模型中影响较小的部分舍弃。
如Compressing BERT，还有结构化剪枝 LayerDrop，其在训练时进行Dropout，预测时再剪掉Layer，
不像知识蒸馏需要提前固定student模型的尺寸大小。</li>
  <li>quantization（量化）：将高精度模型用低精度来表示；
如Q-BERT和Q8BERT，量化通常需要兼容的硬件。</li>
  <li>parameter sharing （参数共享）：相似模型单元间的参数共享；
ALBERT主要是通过矩阵分解和跨层参数共享来做到对参数量的减少。</li>
  <li>module replacing（模块替换）：
BERT-of-Theseus根据伯努利分布进行采样，决定使用原始的大模型模块还是小模型，只使用task loss。</li>
  <li>knowledge distillation （知识蒸馏）：通过一些优化目标从大型、知识丰富、
fixed的teacher模型学习一个小型的student模型。蒸馏机制主要分为3种类型：
①从软标签蒸馏：DistilBERT、EnsembleBERT
②从其他知识蒸馏：TinyBERT、BERT-PKD、MobileBERT 、 MiniLM 、DualTrain
③蒸馏到其他结构：Distilled-BiLSTM</li>
</ul>

<h4 id="3多模态">3、多模态</h4>

<p>随着PTMs在NLP领域的成功，许多研究者开始关注多模态领域的PTMs，
主要为通用的视觉和语言特征编码表示而设计。多模态的PTMs在一些庞大的跨模式数据语料库
（带有文字的语音、视频、图像）上进行了预训练，如带有文字的语音、视频、图像等，
主要有 VideoBERT、CBT 、UniViLM、 ViL-BERT 、 LXMERT、 VisualBERT 、 B2T2、
Unicoder-VL 、UNITER、 VL-BERT 、 SpeechBERT。</p>

<h4 id="4领域预训练">4、领域预训练</h4>

<p>大多数PTM都在诸如Wikipedia的通用语料中训练，而在领域化的特定场景会收到限制。如基于生物医学文本的BioBERT，
基于科学文本的SciBERT，基于临床文本的Clinical-BERT。一些工作还尝试将PTMs适应目标领域的应用，
如医疗实体标准化、专利分类PatentBERT、情感分析SentiLR关键词提取。</p>

<h4 id="5多语言和特定语言">5、多语言和特定语言</h4>

<p>学习跨语言共享的多语言文本表示形式对于许多跨语言的NLP任务起着重要的作用。</p>

<p>Multilingual-BERT在104种 Wikipedia文本上进行MLM训练（共享词表），每个训练样本都是单语言文档，
没有专门设计的跨语言目标，也没有任何跨语言数据，M-BERT也可以很好的执行跨语言任务。
XLM通过融合跨语言任务（翻译语言模型）改进了M-BERT，该任务通过拼接平行语料句子对进行MLM训练。</p>

<p>Unicoder提出了3种跨语言预训练任务：</p>
<ul>
  <li>cross-lingual word recovery；</li>
  <li>cross-lingual paraphrase classification;</li>
  <li>cross-lingual masked language model.
虽然多语言的PTMs在跨语言上任务表现良好，但用单一语言训练的PTMs明显好于多语言的PTMs。
此外一些单语言的PTMs被提出：BERT-wwm， ZEN, NEZHA , ERNIE-Baidu, BERTje, CamemBERT, FlauBERT, RobBERT。</li>
</ul>

<h3 id="对ptms进行迁移学习">对PTMs进行迁移学习</h3>

<p>PTMs从大型语料库中获取通用语言知识，如何有效地将其知识适应下游任务是一个关键问题。
迁移学习的方式主要有归纳迁移（顺序迁移学习、多任务学习）、领域自适应（转导迁移）、跨语言学习等。
NLP中PTMs的迁移方式是顺序迁移学习。</p>

<h4 id="如何迁移">如何迁移？</h4>
<ul>
  <li>
    <p>选择合适的预训练任务：语言模型是PTM是最为流行的预训练任务；同的预训练任务有其自身的偏置，
并且对不同的任务会产生不同的效果。例如，NSP任务可以使诸如问答（QA）和自然语言推论（NLI）之类的下游任务受益。</p>
  </li>
  <li>
    <p>选择合适的模型架构：例如BERT采用的MLM策略和Transformer-Encoder结构，
导致其不适合直接处理生成任务。</p>
  </li>
  <li>
    <p>选择合适的数据：下游任务的数据应该近似于PTMs的预训练任务，
现在已有有很多现成的PTMs可以方便地用于各种特定领域或特定语言的下游任务。</p>
  </li>
  <li>
    <p>选择合适的layers进行transfer：主要包括Embedding迁移、top layer迁移和all layer迁移。
如word2vec和Glove可采用Embedding迁移，BERT可采用top layer迁移，Elmo可采用all layer迁移。</p>
  </li>
  <li>
    <p>特征集成还是fine-tune？对于特征集成预训练参数是freeze的，而fine-tune是unfreeze的。
特征集成方式却需要特定任务的体系结构，fine-tune方法通常比特征提取方法更为通用和方便。</p>
  </li>
</ul>

<h4 id="fine-tune策略">fine-tune策略：</h4>
<p>通过更好的微调策略进一步激发PTMs性能</p>

<p>两阶段fine-tune策略：如第一阶段对中间任务或语料进行finetune，第二阶段再对目标任务fine-tune。
第一阶段通常可根据特定任务的数据继续进行fine-tune预训练。</p>

<p>多任务fine-tune：MTDNN在多任务学习框架下对BERT进行了fine-tune，这表明多任务学习和预训练是互补的技术。</p>

<p>采取额外的适配器：fine-tune的主要缺点是其参数效率低，每个下游任务都有自己的fine-tune参数。
因此，更好的解决方案是在固定原始参数的同时，将一些可fine-tune的适配器注入PTMs。</p>

<p>逐层阶段：逐渐冻结而不是同时对所有层进行fine-tune，也是一种有效的fine-tune策略。</p>

<h3 id="比较典型的基于语言模型的预训练模型发展脉络">比较典型的基于语言模型的预训练模型发展脉络</h3>

<table>
  <thead>
    <tr>
      <th>模型名称</th>
      <th>特征提取模块</th>
      <th>预训练任务</th>
      <th>语言模型</th>
      <th>特点</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ELMO</td>
      <td>BiLSTM</td>
      <td>LM</td>
      <td>单向拼接</td>
      <td>首次真正的将预训练的思想进行了实践</td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>Transformer</td>
      <td>LM</td>
      <td>单向</td>
      <td>统一下游微调任务的形式，证明了transformer结构在预训练任务中的有效性</td>
    </tr>
    <tr>
      <td>BERT</td>
      <td>Transformer</td>
      <td>MLM，NSP</td>
      <td>双向</td>
      <td>创新性的提出MSM任务，首次事先了双向的语言模型</td>
    </tr>
    <tr>
      <td>GPT2.0</td>
      <td>Transformer</td>
      <td>LM</td>
      <td>单向</td>
      <td>在生成任务中取的良好的结果</td>
    </tr>
    <tr>
      <td>MASS</td>
      <td>Transformer</td>
      <td>LM，MLM</td>
      <td>单向，双向</td>
      <td>将序列任务引入到预训练过程</td>
    </tr>
    <tr>
      <td>UNILM</td>
      <td>Transformer</td>
      <td>LM，MLM，S2SLM</td>
      <td>单向，双向</td>
      <td>灵活的运用mask矩阵，实现仅仅用Encoder来作序列任务</td>
    </tr>
    <tr>
      <td>MTDNN</td>
      <td>Transformer</td>
      <td>MLM</td>
      <td>双向</td>
      <td>在下游阶段引入了多任务学习</td>
    </tr>
    <tr>
      <td>Transformer-XL</td>
      <td>Transformer-XL</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>XLNET</td>
      <td>Transformer-XL</td>
      <td>Perm-LM</td>
      <td>双向</td>
      <td>排列语言模型，双注意流</td>
    </tr>
    <tr>
      <td>ALBERT</td>
      <td>Transformer</td>
      <td>MLM</td>
      <td>双向</td>
      <td>对BERT进行了针对性的模型压缩，保留了尽可能大的性能同时精简了模型</td>
    </tr>
    <tr>
      <td>RoBERTa</td>
      <td>Transformer</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>DistillBERT</td>
      <td>Transformer</td>
      <td> </td>
      <td> </td>
      <td>对BERT进行了模型蒸馏，从而达到模型压缩的目的</td>
    </tr>
  </tbody>
</table>

<h4 id="elmodeep-contextualized-word-representations">ELMo:Deep contextualized word representations</h4>
<p>ELMo(Embeddings from Langauge Model)的思想非常简单，通过双向拼接的LSTM结构作为模型的特征提取表示模块，
然后使用LM作为预训练第一个阶段的任务，为了将下游不同的任务融合进去，
论文在上层融合了不同的NLP任务，需要注意的是，作者在融合不同任务的时候，
并没有统一的使用最顶层的向量表示，而是针对不同的任务使用了不同层的向量表示。</p>

<p>elmo的优点在于初步解决了之前的词向量表示中所不能解决的一词多义的问题，
因为每个词向量表示都是根据其周围的词在句子中动态获得的。</p>

<h4 id="gptimproving-language-understandingby-generative-pre-training">GPT:Improving Language Understandingby Generative Pre-Training</h4>
<p>GPT是Open AI的工作，开创性的使用了Transformer来作为预训练模型的特征提取模块，
在NLU相关的下游任务中取得了非常好的效果。模型也是十分简洁的，
预训练部分使用单向的LM，微调阶段也是十分简洁，将一些非序列而任务融合到了一个框架中去，
采用了分隔符标记拼接的方式来统一处理。</p>

<h4 id="gpt20language-models-are-unsupervised-multitask-learners">GPT2.0:Language Models are Unsupervised Multitask Learners</h4>
<p>由于GPT2.0在GPT基础上做的，除了使用了多任务以及超大预训练数据集以及超大的模型之外，
在思想上并没有很大的创新</p>

<h4 id="gpt30-language-models-are-few-shot-learners">GPT3.0 Language Models are Few-Shot Learners</h4>

<p>GPT-3依旧延续自己的单向语言模型训练方式，只不过这次把模型尺寸增大到了1750亿，
并且使用45TB数据进行训练。同时设置了各种size的模型进行对比。</p>

<p>对于所有任务，应用GPT-3无需进行任何梯度更新或微调，而仅通过与模型的文本交互指定任务和少量演示即可。 
GPT-3在许多NLP数据集上均具有出色的性能，包括翻译，问题解答和完形填空任务，
以及一些需要即时推理或领域适应的任务。</p>

<h4 id="bert-pre-training-of-deep-bidirectional-transformers-for-lanuage-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Lanuage Understanding</h4>
<p>这里稍微说下BERT用于预训练中的MLM任务，以及为什么可以称之为双向的语言模型。理想情况下，
我们都希望一个词的语义特征表示是由其上下文共同编码决定的。
显然上面基于链式法则的单向语言模型是不能够学习到双向语义表示的，
因为在预测xt时，无论是正向的语言模型还是反向的语言模型，都只由该时刻一侧的序列决定，
所以模型自然不能将双向的信息编码到中心词中去。而BERT通过借鉴close-test这样的任务思想，
借助Self-Attention Mask，巧妙的解决了上述问题。</p>
<h4 id="massmasked-sequence-to-sequence-pre-training-for-language-generation">MASS:Masked Sequence to Sequence Pre-training for Language Generation</h4>
<p>要注意的是，包括BERT在内的预训练模型大多都在一些NLU任务上表现优异，这些任务的基本都是一些序列分类任务，
当然，也有很多人在一些序列标注任务上做过一些尝试，但是像机器翻译，文本摘要或者其他的序列生成任务，
这些预训练模型便显的力不从心了。</p>

<p>根本的原因是:这些模型都只是由一个Encoder结构组成，
对于标注任务单独一个Encoder还是可以解决的，但是像序列生成这样的任务便无能为力了。
MASS就是为了解决这一问题而提出的预训练模型。实际上MASS的思想很简单，既然一个单独的Encoder不能解决，
那么再叠加一个Decoder作为解码部分不就可以了，因此MASS在预训练模型部分添加了Decoder结构，</p>

<h4 id="unilmunified-language-model-pre-training">UniLM:Unified Language Model Pre-training</h4>
<p>UniLM的巧妙之处在于将上面提到的三种语言模型都集成到了一个模型中去，而且仅仅用了Encoder结构。</p>

<p>预训练采用了1/3的双向语言模型，1/3的序列到序列语言模型、各占1/6的从左到右的以及从右到左的单向语言模型。
同时也像BERT一样集成了NSP任务。</p>
:ET