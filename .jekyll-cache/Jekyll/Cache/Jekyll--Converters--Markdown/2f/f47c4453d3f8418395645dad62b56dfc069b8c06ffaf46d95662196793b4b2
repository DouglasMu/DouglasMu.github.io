I"<h2 id="预训练模型总结">预训练模型总结</h2>

<h3 id="什么是预训练模型">什么是预训练模型？</h3>

<ul>
  <li>
    <p>迁移学习(Transfer learning) 顾名思义就是把已训练好的模型（预训练模型）参数
迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务都是存在相关性的，
所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）
通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。</p>
  </li>
  <li>
    <p>pre-training就是把一个已经训练好的模型参数应用到另一个类似任务上作为初始参数，这样之前训练模型的过程就叫做预训练</p>
  </li>
  <li>
    <p>fine tuning就是在训练新任务的过程中慢慢调整参数（微调）</p>
  </li>
</ul>

<h3 id="预训练模型有什么作用">预训练模型有什么作用？</h3>

<p>深度学习时代，为了充分训练深层模型参数并防止过拟合，通常需要更多标注数据喂养。在NLP领域，标注数据更是一个昂贵资源。PTMs从大量无标注数据中进行预训练使许多NLP任务获得显著的性能提升。总的来看，预训练模型PTMs的优势包括：</p>

<ul>
  <li>在庞大的无标注数据上进行预训练可以获取更通用的语言表示，并有利于下游任务；</li>
  <li>为模型提供了一个更好的初始化参数，在目标任务上具备更好的泛化性能、并加速收敛；</li>
  <li>是一种有效的正则化手段，避免在小数据集上过拟合（一个随机初始化的深层模型容易对小数据集过拟合）</li>
</ul>

<h3 id="预训练模型的两大范式">预训练模型的两大范式</h3>
:ET