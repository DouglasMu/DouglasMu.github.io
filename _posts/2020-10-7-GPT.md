---
layout: post
title:  GPT预训练模型
categories: python
tags:  深度学习 预训练模型
author: Keyon
---
* content
{:toc}

GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。
GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，
第二阶段通过Fine-tuning的模式解决下游任务。

![gpt_1.png](https://i.loli.net/2020/10/07/lucAkm81qT2GoQM.png)









上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：

首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，
这个选择很明显是很明智的；

其次，GPT的预训练虽然仍然是以语言模型作为目标任务，
但是采用的是单向的语言模型，所谓“单向”的含义是指：
语言模型训练的任务目标是根据Wi单词的上下文去正确预测单词Wi ， 
Wi之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

ELMO在做语言模型预训练的时候，预测单词Wi同时使用了上文和下文，
而GPT则只采用Context-before这个单词的上文来进行预测，
而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，
它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，
比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。
如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。